{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import json\n",
    "import glob\n",
    "import sent2vec\n",
    "import nltk.data\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load nltk NLP files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "# instantiate progress bar for notebook\n",
    "tqdm.pandas(tqdm_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to load articles repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperLoader():\n",
    "    \"\"\"\n",
    "    Loads, parses and merges metadata for papers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, no_bib=True):\n",
    "        \"\"\"\n",
    "        Initializes PaperLoader class to read all .json files from root_directory\n",
    "            \n",
    "            no_bib: if true, clean noisy sections with bibliographies\n",
    "            root_dir: root directory for papers\n",
    "        \"\"\"\n",
    "        self.ROOT_DIR = root_dir\n",
    "        self.JSON_FILES = glob.glob(f'{root}/**/*.json', recursive=True)\n",
    "        self.PAPERS_COLUMN = {\n",
    "            \"doc_id\": [None],\n",
    "            \"title\": [None],\n",
    "            \"abstract\": [None],\n",
    "            \"text_body\": [None]\n",
    "        }\n",
    "        self.PAPERS_DF = None\n",
    "        self.PAPERS_SECTION_DF = None\n",
    "        self.NO_BIB = no_bib\n",
    "\n",
    "    \n",
    "    def __clean_bib(self, body_text, thres):\n",
    "        \"\"\"\n",
    "        Removes sections with more than 5 URL/DOI/HTTP instances\n",
    "            \n",
    "            body_text: array of dictionaries for text_body\n",
    "            thres: number of hyperlinks tolerated before removal \n",
    "        \"\"\"\n",
    "        # Sometimes, the text body has duplicate sections consecutively.\n",
    "        merged_body = []\n",
    "        for segment in body_text:\n",
    "            # We will combine these duplicate sections\n",
    "            if len(merged_body) > 0:\n",
    "                if merged_body[-1]['section'] == segment['section']:\n",
    "                    merged_body[-1]['text'] += '\\n' + segment['text']\n",
    "                    continue\n",
    "            merged_body.append(segment)\n",
    "\n",
    "        merged_body = [\n",
    "            segment for segment in merged_body\n",
    "            if len(re.findall(\"(http|doi|www)\", segment['text'])) <= thres\n",
    "        ]\n",
    "        return merged_body\n",
    "       \n",
    "    def create_paper_df(self):\n",
    "        \"\"\"\n",
    "        Creates a Pandas DataFrame from all json files in root_directory\n",
    "        Each json file represents a paper. \n",
    "        Features extracted are: doc_id, title, abstract, text_body\n",
    "        \"\"\"\n",
    "        df_list = []\n",
    "        df_sent = pd.DataFrame()\n",
    "        df_section = pd.DataFrame()\n",
    "        for i in tqdm(range(len(self.JSON_FILES))):\n",
    "            file_name = self.JSON_FILES[i]\n",
    "            \n",
    "            #Initialize row for returned df. Each row represents a paper\n",
    "            row = {x: None for x in self.PAPERS_COLUMN}\n",
    "            sent_row = {x: None for x in self.PAPERS_COLUMN}\n",
    "            with open(file_name) as json_data:\n",
    "                data = json.load(json_data)\n",
    "\n",
    "                row['doc_id'] = data['paper_id']\n",
    "                row['title'] = data['metadata']['title']\n",
    "                \n",
    "                # If title is empty, we skip the paper\n",
    "                if len(row['title']) <= 2:\n",
    "                    continue\n",
    "\n",
    "                # If a paper does not have an abstract of a body, we will skip it\n",
    "                if ('abstract' not in data or 'body_text' not in data):\n",
    "                    continue\n",
    "                else:\n",
    "                    # Now need all of the abstract. Put it all in\n",
    "                    # a list then use str.join() \n",
    "                    abstract_list = [abst['text'] for abst in data['abstract']]\n",
    "                    abstract = \"\\n \".join(abstract_list)\n",
    "\n",
    "                # Skip the paper if abstract is empty\n",
    "                if len(abstract) <= 2:\n",
    "                    continue\n",
    "\n",
    "                row['abstract'] = abstract\n",
    "                    \n",
    "                # And lastly the body of the text.\n",
    "                # These clauses check if the user wants to clean up references\n",
    "                if self.NO_BIB:\n",
    "                    body_list = self.__clean_bib(data['body_text'], 4)\n",
    "                else:\n",
    "                    body_list = [bt for bt in data['body_text']]\n",
    "\n",
    "                row['text_body'] = body_list\n",
    "                sent_row['doc_id'] = data['paper_id']\n",
    "                temp = pd.DataFrame(body_list)\n",
    "                temp['doc_id'] = data['paper_id']\n",
    "                df_section= df_section.append(temp)\n",
    "                df_list.append(row)\n",
    "        # create final dataframe\n",
    "        self.PAPERS_DF = pd.DataFrame(df_list)\n",
    "        self.PAPERS_SECTION_DF =df_section\n",
    "        \n",
    "    def merge_metadata(self, metadata = 'metadata.csv'):\n",
    "        \"\"\"\n",
    "            Joins paper information with information on journal for paper,\n",
    "            authors, doi and published date  \n",
    "                metadata: path to csv file containing metadata\n",
    "        \"\"\"\n",
    "        metadata_df = pd.read_csv(self.ROOT_DIR + metadata)\n",
    "        metadata_df = metadata_df.loc[:, ['sha', 'publish_time', 'authors', 'journal', 'doi']]\n",
    "        self.PAPERS_DF = self.PAPERS_DF.merge(metadata_df,left_on='doc_id',right_on='sha', how='inner')\n",
    "\n",
    "    def get_df(self):\n",
    "        \"\"\"\n",
    "        Returns processed dataframe\n",
    "        \"\"\"\n",
    "        self.PAPERS_DF = self.PAPERS_DF.dropna(subset=['abstract', 'text_body'])\n",
    "        self.PAPERS_SECTION_DF = self.PAPERS_SECTION_DF.dropna(subset=['doc_id', 'section','text'])\n",
    "        self.PAPERS_SECTION_DF = self.PAPERS_SECTION_DF[['doc_id', 'section','text']]\n",
    "        return self.PAPERS_DF,self.PAPERS_SECTION_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to generate sentence dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_df(docid,sec,text):\n",
    "    global paper_sent_df\n",
    "    temp_sent = pd.DataFrame(tokenizer.tokenize(text), columns=['sentence'])\n",
    "    temp_sent['doc_id'] = docid\n",
    "    temp_sent['section'] = sec\n",
    "    paper_sent_df= paper_sent_df.append(temp_sent)\n",
    "    return temp_sent\n",
    "\n",
    "#df_section= df_section.append(temp)\n",
    "#for section_row in temp.itertuples():\n",
    "#    temp_sent = pd.DataFrame(tokenizer.tokenize(section_row.text), columns=['sentence'])\n",
    "#    temp_sent['doc_id'] = data['paper_id']\n",
    "#    temp_sent['section'] = section_row.section\n",
    "#    df_sent= df_sent.append(temp_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Aricles into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:29<00:00, 125.59it/s]\n"
     ]
    }
   ],
   "source": [
    "root = \"./data/comm_use_subset/\"\n",
    "#meta=\"./data/metadata.csv\"\n",
    "paper_loader = PaperLoader(root)\n",
    "paper_loader.create_paper_df()\n",
    "paper_loader.merge_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df,paper_sec_df = paper_loader.get_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sentence dataframe from section data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107535/107535 [1:25:35<00:00, 20.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                  s...\n",
       "1                                                 se...\n",
       "2                                                 se...\n",
       "3                                                  s...\n",
       "4                                                 se...\n",
       "                            ...                        \n",
       "8                                                 se...\n",
       "9                                                 se...\n",
       "10                                                se...\n",
       "11                                                 s...\n",
       "12                                                se...\n",
       "Length: 107535, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_sent_df = pd.DataFrame()\n",
    "df= paper_sec_df.progress_apply(lambda x: generate_sentence_df(x['doc_id'], x['section'],x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64492"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_sec_df.section.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    1619203\n",
       "doc_id      1619203\n",
       "section     1619203\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_sent_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc_id          7862\n",
       "title           7862\n",
       "abstract        7862\n",
       "text_body       7862\n",
       "sha             7862\n",
       "publish_time    7862\n",
       "authors         7854\n",
       "journal         7793\n",
       "doi             7811\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords for covid-19\n",
    "cov_list = [\n",
    "    'novel coronavi',\n",
    "    'covid',\n",
    "    'cov_2',\n",
    "    'cord-19',\n",
    "    'cord 19',\n",
    "    '2019-nCoV',\n",
    "    '2019 ncov',\n",
    "    '2019 cov',\n",
    "    'wuhan coronavi',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fucntions for filtering relevant articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevantFilter():\n",
    "    \n",
    "    def __init__(self, keywords, year='2019'):\n",
    "        \"\"\"\n",
    "        constructor for RelevantFilter\n",
    "            keywords: keywords to filter for\n",
    "            year: papers written before this year will be discarded\n",
    "        \"\"\"\n",
    "        self.KEYWORDS = keywords\n",
    "        self.YEAR = year\n",
    "\n",
    "    def extract_recent(self, df):\n",
    "        \"\"\"\n",
    "        extracts documents published on or after self.YEAR\n",
    "        \"\"\"\n",
    "        return df[df['publish_time'] >= self.YEAR]\n",
    "\n",
    "    def filter_papers(self, df):\n",
    "        \"\"\"\n",
    "        Filters for papers whose title have mention of \n",
    "        any of the terms in self.KEYWORDS\n",
    "        \"\"\"\n",
    "        pattern = re.compile('(' + \"|\".join(self.KEYWORDS) + ')',\n",
    "                                 re.IGNORECASE)\n",
    "        # We will filter for rows with one or more matches \n",
    "        # for title and covid keywords\n",
    "        df = df[df['title'].apply(lambda x: \n",
    "                                  len(pattern.findall(x)) >= 1\n",
    "                                  if x else False)]\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter covid19 related articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_filter = RelevantFilter(cov_list, '2019')\n",
    "covid_df = covid_filter.filter_papers(papers_df)\n",
    "covid_df = covid_filter.extract_recent(covid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "convid_sent_df = paper_sent_df[paper_sent_df.doc_id.isin(covid_df['doc_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convid_sent_df.doc_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Trained BioSentVec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model successfully loaded\n"
     ]
    }
   ],
   "source": [
    "model_path = 'BioSentVec_PubMed_MIMICIII-bigram_d700.bin'\n",
    "model = sent2vec.Sent2vecModel()\n",
    "try:\n",
    "    model.load_model(model_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print('model successfully loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre process sentence - basic text clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_sentence(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate BioSentVec embedding vectore for sentence corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = model.embed_sentences(convid_sent_df['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embedding vector for question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.embed_sentence('Physical science of the coronavirus') \n",
    "#embs = model.embed_sentences([\"first sentence .\", \"another sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Train KNN model with sentence embedding vecotor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict answer sentence for question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = nbrs.kneighbors(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6090,  114]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.1479283 , 4.29738979]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "convid_sent_df =convid_sent_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'da81f0d3a12ab7faa09148acb6564271474e9e02'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convid_sent_df['doc_id'].iloc[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5ba8056230c17ec133169d79aacf61ed7d4b458b'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convid_sent_df['doc_id'].iloc[6090]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers as output from KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  The novel coronavirus outbreak in Wuhan, China\n",
      "Abstract:  The novel coronavirus (2019-nCoV, or COVID-19)...\n",
      "Section of the article:  Introduction\n",
      "Answer:                                                                                                                                                                                                sentence\n",
      " But most pressingly as the global outbreak continues to grow, can we develop effective vaccine and therapeutic strategies to treat not only this epidemic but any future coronavirus spillover events?\n",
      "                                                                                                                                  The COVID-19 has then rapidly spread to all over China and the world.\n"
     ]
    }
   ],
   "source": [
    "print(\"Title:\",(covid_df[covid_df.doc_id =='5ba8056230c17ec133169d79aacf61ed7d4b458b']['title']).to_string(index=False))\n",
    "print(\"Abstract:\",(covid_df[covid_df.doc_id =='5ba8056230c17ec133169d79aacf61ed7d4b458b']['abstract']).to_string(index=False))\n",
    "print(\"Section of the article:\",(convid_sent_df[['section']].iloc[6091]).to_string(index=False))\n",
    "print(\"Answer:\",(convid_sent_df[['sentence']].iloc[[6089,6091]]).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Molecular and serological investigation of 201...\n",
      "Abstract:  In December 2019, a novel coronavirus (2019-nC...\n",
      "Section of the article:  Introduction\n",
      "Answer:                                                                                                                          sentence\n",
      "           Coronaviruses (CoVs) belong to the subfamily Orthocoronavirinae in the family Coronaviridae and the order Nidovirales.\n",
      "                 A human coronavirus (SARS-CoV) caused the severe acute respiratory syndrome coronavirus (SARS) outbreak in 2003.\n",
      " Most recently, an SARS-related CoV was implicated as the etiological agent responsible for the outbreak in Wuhan, central China.\n"
     ]
    }
   ],
   "source": [
    "print(\"Title:\",(covid_df[covid_df.doc_id =='da81f0d3a12ab7faa09148acb6564271474e9e02']['title']).to_string(index=False))\n",
    "print(\"Abstract:\",(covid_df[covid_df.doc_id =='da81f0d3a12ab7faa09148acb6564271474e9e02']['abstract']).to_string(index=False))\n",
    "print(\"Section of the article:\",(convid_sent_df[['section']].iloc[114]).to_string(index=False))\n",
    "print(\"Answer:\",(convid_sent_df[['sentence']].iloc[[113,114,115]]).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
